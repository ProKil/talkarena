import Link from 'next/link'
import Image from 'next/image'
import { Lumiflex } from 'uvcanvas'
import { Button } from "../components/ui/button"
import { MoveRight } from "lucide-react"

import React, { useState } from 'react';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';
import ModelPerformanceChart from '../components/static_performance';
import KendallTauDistanceChart from '../components/interactive_static_correlation';

import { Hero } from "@/components/hero";
import { GradioEmbed } from "@/components/gradio";
import { ResearchTable } from "@/components/tablehero";
import { Callout, Bleed } from 'nextra/components'

<Hero />

# Introduction
As AI Assistants' capability expands beyond text into other modalities like audio,
they enable new forms of interactions between humans and AI. On the other hand,
most benchmarks for audio processing primarily focus on transcribing and analyzing user speech.
We introduce **Talk Arena**, an interactive open platform to evaluate Large Audio Models through *interactions with
users in real-world settings*. It helps to assess whether previous static benchmarks are valuable measures of model quality,
or whether there is need for a new evaluation paradigm for Large Audio Models.
We evaluate large audio models using *18 speech comprehension datasets* and **Talk Arena**.

<GradioEmbed />

# Overview

![Comparison between Static Evaluation and Talk Arena](/overview_arena.png)

Recent efforts towards creating multimodal models have resulted in LLMs capable of processing audio inputs such as speech. Speech is a low-friction interface which expands social and phonetic interaction opportunities with end users. Prior work has benchmarked audio models on a set of disjoint static audio tests such as sarcasm or humor detection. However such static benchmarks lack the complex dynamics of real user interactions and preferences. Inspired by arena-style evaluations for text LLMs we introduce Talk Arena, an open platform for evaluating Large Audio Models with pairwise human preferences. Talk Arena helps to reveal insights on:

* What use cases users are exploring with large audio models? We can analyze user queries from the wild and compare the use case difference with traditional use cases of text LLMs.
* Which Large Language Model users prefer the most? Users vote their preferences with self-initiated prompts, which better reflects the actual user experience.
* Are static speech comprehension benchmarks predictive user preferences in interactive settings? It helps to reveal the gap between the mainstream evaluation method for audio models and actual user preferences.

# Static Evaluation

## Tasks and Datasets

We select 18 speech comprehension benchmark and perform evaluation for 10 different large audio models.

A wide range of tasks are covered, which include Humor Detection, Sarcasm Detection, Intent Detection, Relationship Classification, Gender Classification, Age Classification, Accent Classification, Speech Grounding, Language Identification, Speech Entity Recognition, Speech Question Answering, and Speech Instruction Following. They can be categorized into three broad categories of Speaker Cognitive State, Speaker Identity, and Speech Content Understanding.

<ResearchTable />



## Result Analysis
To ensure robustness, we report the average of model performance using three different prompt variations.
For `public_sg_speech`, `openhermes`, and `alpaca` datasets, we report the
<a href="https://arxiv.org/html/2402.11161v4">cfm</a> metric. For other tasks, we report the macro F1 scores.

In general, close sourced models like <a href="https://arxiv.org/abs/2312.11805">Gemini</a> and
<a href="https://arxiv.org/abs/2312.11805">GPT4o</a> generally top the leaderboard:
<a href="https://arxiv.org/abs/2312.11805">Gemini</a> has the highest performance on SLURP intent classification,
MELD emotion recognition, CN_college_listen and <a href="https://arxiv.org/abs/2312.11805">GPT4o</a>
performs the best on MUSTARD, IEMOCAP, CallHome, Fairspeech (age classification), and
Commonvoice (accent classification) datasets. They also perform relatively well despite not achieving best
performance, showing <b>good generalization and transferability in them</b>.

Among the open-sourced models, <Link href="https://arxiv.org/abs/2407.10759">Qwen2-Audio</Link> demonstrates outstanding
performance on SpeechQA and Gender/Age classification tasks and <a href="https://arxiv.org/abs/2410.02678">DiVA</a>
shows excellent humor detection and speech instruction following capability that outperforms all other models.
They also show relatively good performance on other tasks, demonstrating <b>good generalizability</b>.
<a href="https://arxiv.org/pdf/2309.05519">NextGPT</a> and <a href="https://arxiv.org/abs/2305.16355">PandaGPT</a>
perform relatively worse, especially for tasks like intent and emotion recognition, accent recognition,
and instruction following. They share similar encoder architecture (ImageBind) and this suggests
<b>the limitation of using ImageBind for encoding audio features</b>.

We also perform evaluation for the sequential pipeline of <a href="https://arxiv.org/abs/2212.04356">Whisper</a>
plus <a href="https://arxiv.org/abs/2407.21783">Llama3-8B-Instruct</a>.
It shows relatively good performance for tasks like emotion recognition and speech QA, which means some of the data
instances can be inferred from content only. However, for each and every task there are speech models outperforming
the whisper+llama3 pipeline. This suggests that some information like emotion, relationship, and sarcasm can be
<b>embedded in vocal cues</b> and <b>requires understanding beyond content</b>.


<ModelPerformanceChart />

## Interactive Evaluation

### User Preference

As an initial effort, we collected a total 5000 votes using Talk Arena for pairwise comparisons among GPT4o,
Gemini-1.5-pro, Typhoon, Qwen2-Audio and DiVA, which are top performing models from the results of static evaluation.
For each of the ten combinations, we collect 500 votes from more than 50 different crowdworkers. A tie will be counted as half vote to both models.

![User Study (tie cases counted as half vote)](/user_study_2.png)

We applied Bradley Terry model to pariwise voting results to get a ranking for the five models tested.
The final result shows a ranking of DiVA, GPT4o, Gemini-1.5-pro, Qwen2-Audio, Typhoon-1.5
(most preferred to less preferred).
![Bradley Terry](/bradley_terry.svg)

### Comparison with Static Evaluation

We compare the user preference result in interactive evaluation to that of static evaluation by computing the top-k Kendall Tau Distance between rank in static evaluation and that in interactive evaluation:

<KendallTauDistanceChart />

Here are some observations:
1. **None** of the static bench reflects exactly the same rank in interactive eval
2. Ranks on **emotion recognition** and **language detection** benchmarks are most similar to that in interactive eval
3. Ranks on **gender detection** and **nuanced intent (humor, sarcasm) detection** are not so correlated with that in interactive eval

## Looking Forward

**Moving Forward.** We are inspired by the ways that the Chatbot Arena has rapidly accelerated the research focus on real-world usage of Chatbot models. As we look ahead, we hope that we can similarly focus the development of Speech LLMs on what users want, rather than what current benchmarks can measure.
- **Bringing Human Preferences to Audio** While the current platform does not store query data, we are excited to start discussions with the community about what data the general public is willing to contribute. Unlike text, speech data is inherently personally identifiable. This means that both consent to data contribution and the careful release of this data is especially important. By working in the open, we hope to find a way to bring a wide range of perspectives to make Large Audio Models useful for everyone who wants to use them.
- **Free-Form Conversational Data** Unlike text chat, spoken conversations are often free-flowing. This means there can be many turns in a short period of time and speakers can interrupt one another! This is part of what makes the modality desirable, but it presents challenges for Arena-style evaluation. We're excited to dig into how we can extend this user-centric evaluation paradigm to all the new ways people are chatting to virtual assistants with speech.
- **Building Better Static Benchmarks** Human evaluation is not a feasible way to guide model development. It is slow and often expensive. We hope that the insights derived from our interactive evaluation can help guide the creation of new static benchmarks that better correlate with real-world usage and user preference.

<Callout type="info">
Ethics and Disclosure

This study has been approved by the Institutional Review Board (IRB) at the researchers' institution, and we obtained participant consent with a standard institutional consent form. 

For the Prolific study, we stored users queries as well as rationale about why they picked their votes. While our focus is on understanding model behaviours that are desirable, doing this analysis required storing this personally identifiable information. As such, this study was approved under Expedited Review Categories 6 and 7 for the collection of voice for research purposes and research employing survey methods. All participants were paid 15 dollars an hour on the prolific platform.

The public platform, on the other hand, **does not store any information about you or your queries for further research** or release. All recordings are erased after you submit your votes and vote data are stored to understand model performance, but no information about your requests is stored. The public platform sends data to third-party APIs from OpenAI and Google. In both cases, we utilize paid APIs for which the terms of service dictate that your data will not be used for training or stored for more than 30 days. Based on the lack of identifiable information and the focus on model rankings, the Stanford IRB has determined that this public platform is not human subects research as defined in 45 CFR 46.102(e).

</Callout>

## Acknowledgements
We appreciate the computing support provided by Stanford HAI Google Cloud Credit Program and credit support provided by Google Gemini. We also would like to thank Qwen team for their help with building model API endpoint.

## Cite us

If you find this work useful, please cite us:

```bibtex
  @misc{talkarena2024,
      title={Talk Arena: Interactive Evaluation of Large Audio Models},
      author={Minzhi Li and Will Held and Michael Ryan and Hao Zhu and Kunat Pipatanakul and Potsawee Manakul and Diyi Yang},
      year={2024}
    }
```

## Collaboration
We are open to collaboration! If you are interested in this project, please feel free to contact us at ellamzli@stanford.edu

	
