import Link from 'next/link'
import Image from 'next/image'
import { Lumiflex } from 'uvcanvas'
import { Button } from "../components/ui/button"
import { MoveRight } from "lucide-react"

import React, { useState } from 'react';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';
import ModelPerformanceChart from '../components/static_performance';
import KendallTauDistanceChart from '../components/interactive_static_correlation';

import { Hero } from "@/components/hero";
import { GradioEmbed } from "@/components/gradio";
import { ResearchTable } from "@/components/tablehero";
import { Callout, Bleed } from 'nextra/components'

<Hero />

# Introduction
As AI Assistants' capability expands beyond text into other modalities like audio,
they enable new forms of interactions between humans and AI. On the other hand,
most benchmarks for audio processing primarily focus on transcribing and analyzing user speech.
We introduce **Talk Arena**, an interactive open platform to evaluate Large Audio Models through *interactions with
users in real-world settings*. It helps to assess whether previous static benchmarks are valuable measures of model quality,
or whether there is need for a new evaluation paradigm for Large Audio Models.
We evaluate large audio models using *18 speech comprehension datasets* and **Talk Arena**.

<GradioEmbed />

# Overview

![Comparison between Static Evaluation and Talk Arena](/overview_arena.png)

Recent efforts towards creating multimodal models have resulted in LLMs capable of processing audio inputs such as speech. Speech is a low-friction interface which expands social and phonetic interaction opportunities with end users. Prior work has benchmarked audio models on a set of disjoint static audio tests such as sarcasm or humor detection. However such static benchmarks lack the complex dynamics of real user interactions and preferences. Inspired by arena-style evaluations for text LLMs we introduce Talk Arena, an open platform for evaluating Large Audio Models with pairwise human preferences. Talk Arena helps to reveal insights on:

* What use cases users are exploring with large audio models? We can analyze user queries from the wild and compare the use case difference with traditional use cases of text LLMs.
* Which Large Language Model users prefer the most? Users vote their preferences with self-initiated prompts, which better reflects the actual user experience.
* Are static speech comprehension benchmarks predictive user preferences in interactive settings? It helps to reveal the gap between the mainstream evaluation method for audio models and actual user preferences.



# Interactive Evaluation

### User Preference

As an initial effort, we collected a total 5000 votes using Talk Arena for pairwise comparisons among GPT4o,
Gemini-1.5-pro, Typhoon, Qwen2-Audio and DiVA, which are top performing models from the results of static evaluation.
For each of the ten combinations, we collect 500 votes from more than 50 different crowdworkers. A tie will be counted as half vote to both models.

![User Study (tie cases counted as half vote)](/user_study_2.png)

We applied Bradley Terry model to pariwise voting results to get a ranking for the five models tested.
The final result shows a ranking of DiVA, GPT4o, Gemini-1.5-pro, Qwen2-Audio, Typhoon-1.5
(most preferred to less preferred).
![Bradley Terry](/bradley_terry.svg)

### Comparison with Static Evaluation

We compare the user preference result in interactive evaluation to that of static evaluation by computing the top-k Kendall Tau Distance between rank in static evaluation and that in interactive evaluation:

<KendallTauDistanceChart />

Here are some observations:
1. **None** of the static bench reflects exactly the same rank in interactive eval
2. Ranks on **emotion recognition** and **language detection** benchmarks are most similar to that in interactive eval
3. Ranks on **gender detection** and **nuanced intent (humor, sarcasm) detection** are not so correlated with that in interactive eval

## Looking Forward

**Key Insights.** The observed **changes in model rankings** between static and interactive evaluations highlight the limitations of static audio benchmarks in capturing the complexities of real-world interactions. In contrast, interactive evaluations provide a more realistic measure of an audio model's capabilities, revealing strengths and weaknesses that static methods may overlook. **The discrepancies underscore the urgent need for an interactive evaluation platform like Talk Arena** to ensure models are assessed in contexts that reflect their actual use. We call on the research community to prioritize the development of interactive evaluation for audio models, enabling more reliable and actionable insights into performance of different Large Audio Models.

**Moving Forward.** As next steps, we hope to: **1) Scale up the data collection.** We hope to collect more pairwise user preferences from the general public. This will allow us to get insights on user preference among different large audio models with more diverse perspectives. **2) Support multi-turn conversations in the system.** Multi-turn which requires higher adaptability to user feedback and dynamic environments. It also allows evaluation along more dimensions such as consistency of the flow, context handling across turns and so on. It can help us achieve a more comprehensive understanding of audio models’ conversational abilities, ensuring it meets the nuanced demands of real-world interactions. **3) Control data quality when scaling up.** When collecting user preferences from the wild, malicious users may deliberately introduce harmful, biased, or nonsensical inputs, compromising the quality of the evaluation. By identifying and mitigating malicious activity, we hope to maintain the reliability of our evaluation system.

<Callout type="info">
Ethics and Disclosure

This study has been approved by the Institutional Review Board (IRB) at the researchers' institution, and we obtained participant consent with a standard institutional consent form.

</Callout>

## Acknowledgements
We appreciate the computing support provided by Stanford HAI Google Cloud Credit Program and credit support provided by Google Gemini. We also would like to thank Qwen team for their help with building model API endpoint.

## Cite us

If you find this work useful, please cite us:

```bibtex
  @misc{talkarena2024,
      title={Talk Arena: Interactive Evaluation of Large Audio Models},
      author={Minzhi Li and Will Held and Michael Ryan and Hao Zhu and Kunat Pipatanakul and Potsawee Manakul and Diyi Yang},
      year={2024}
    }
```

## Collaboration
We are open to collaboration! If you are interested in this project, please feel free to contact us at ellamzli@stanford.edu

	
