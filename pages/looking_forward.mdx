# Looking Forward

We are inspired by how the Chatbot Arena has rapidly accelerated research on real-world applications of language models in conversational systems. As we look ahead, we aim to similarly focus the development of speech-enabled language models on user needs, rather than limiting innovation to what current benchmarks can measure.

- **Incorporating Human Preferences in Speech Data** While the current platform does not store query data, we are excited to initiate discussions with the community about data contribution frameworks. Unlike text, speech data is inherently personally identifiable. This means that both consent to data contribution and careful data release are especially important. Through open collaboration, we seek to develop approaches that bring diverse perspectives to make speech-enabled language models useful for all potential users.

- **Managing Free-Form Conversational Dynamics ** Unlike text chat, spoken conversations are often free-flowing. This means there can be many turns in a short period of time and speakers can interrupt one another. While these features make the modality compelling, they present challenges for Arena-style evaluation. We are investigating methods to extend user-centric evaluation paradigms to these new patterns of speech-based human-AI interaction.

- **Developing Robust Static Benchmarks** Human evaluation, while valuable, is inefficient for guiding model development due to speed and cost. We anticipate that insights from our interactive evaluation framework will inform the creation of static benchmarks that better correlate with real-world usage patterns and user preferences to mitigate this issues.
