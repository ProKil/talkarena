import Link from 'next/link'
import Image from 'next/image'
import { Lumiflex } from 'uvcanvas'
import { Button } from "../components/ui/button"
import { MoveRight } from "lucide-react"

import React, { useState } from 'react';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';
import ModelPerformanceChart from '../components/static_performance';
import KendallTauDistanceChart from '../components/interactive_static_correlation';

import { Hero } from "@/components/hero";
import { GradioEmbed } from "@/components/gradio";
import { ResearchTable } from "@/components/tablehero";
import { Callout, Bleed } from 'nextra/components'

## Looking Forward

**Moving Forward.** We are inspired by the ways that the Chatbot Arena has rapidly accelerated the research focus on real-world usage of Chatbot models. As we look ahead, we hope that we can similarly focus the development of Speech LLMs on what users want, rather than what current benchmarks can measure.
- **Bringing Human Preferences to Audio** While the current platform does not store query data, we are excited to start discussions with the community about what data the general public is willing to contribute. Unlike text, speech data is inherently personally identifiable. This means that both consent to data contribution and the careful release of this data is especially important. By working in the open, we hope to find a way to bring a wide range of perspectives to make Large Audio Models useful for everyone who wants to use them.
- **Free-Form Conversational Data** Unlike text chat, spoken conversations are often free-flowing. This means there can be many turns in a short period of time and speakers can interrupt one another! This is part of what makes the modality desirable, but it presents challenges for Arena-style evaluation. We're excited to dig into how we can extend this user-centric evaluation paradigm to all the new ways people are chatting to virtual assistants with speech.
- **Building Better Static Benchmarks** Human evaluation is not a feasible way to guide model development. It is slow and often expensive. We hope that the insights derived from our interactive evaluation can help guide the creation of new static benchmarks that better correlate with real-world usage and user preference.
