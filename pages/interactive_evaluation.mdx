import Link from 'next/link'
import Image from 'next/image'
import { Lumiflex } from 'uvcanvas'
import { Button } from "../components/ui/button"
import { MoveRight } from "lucide-react"

import React, { useState } from 'react';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';
import ModelPerformanceChart from '../components/static_performance';
import KendallTauDistanceChart from '../components/interactive_static_correlation';
import ModelComparisonChart from '../components/win_rates';

import { Hero } from "@/components/hero";
import { GradioEmbed } from "@/components/gradio";
import { ResearchTable } from "@/components/tablehero";
import { Callout, Bleed } from 'nextra/components'

# Interactive Evaluation

### User Preference

As an initial effort, we collected a total 5000 votes using Talk Arena for pairwise comparisons among GPT4o,
Gemini-1.5-pro, Typhoon, Qwen2-Audio and DiVA, which are top performing models from the results of static evaluation.
For each of the ten combinations, we collect 500 votes from more than 50 different crowdworkers.

<ModelComparisonChart />

We applied Bradley Terry model to pariwise voting results to get a ranking for the five models tested.
The final result shows a ranking of DiVA, GPT4o, Gemini-1.5-pro, Qwen2-Audio, Typhoon-1.5
(most preferred to less preferred).
|     Model     | Bradley Terry Score  |    Date    | Open-Sourced? | Site  |
|:-------------:|:--------------------:|:----------:|:-------------:|:-----:|
|     üèÖDiVA     |         36.0         | 2Ô∏è‚É£0Ô∏è‚É£2Ô∏è‚É£4Ô∏è‚É£-1Ô∏è‚É£2Ô∏è‚É£-0Ô∏è‚É£1Ô∏è‚É£ |       ‚úÖ       |   [üîó](https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b)   |
|     ü•àGPT4o    |         24.2         | 2Ô∏è‚É£0Ô∏è‚É£2Ô∏è‚É£4Ô∏è‚É£-1Ô∏è‚É£2Ô∏è‚É£-0Ô∏è‚É£1Ô∏è‚É£ |       ‚ùå       |   üîó   |
|    ü•âGemini    |         21.7         | 2Ô∏è‚É£0Ô∏è‚É£2Ô∏è‚É£4Ô∏è‚É£-1Ô∏è‚É£2Ô∏è‚É£-0Ô∏è‚É£1Ô∏è‚É£ |       ‚ùå       |   üîó   |
| 4Ô∏è‚É£ Qwen2 Audio |         14.5         | 2Ô∏è‚É£0Ô∏è‚É£2Ô∏è‚É£4Ô∏è‚É£-1Ô∏è‚É£2Ô∏è‚É£-0Ô∏è‚É£1Ô∏è‚É£ |       ‚úÖ       |   üîó   |
|   5Ô∏è‚É£ Typhoon   |          3.6         | 2Ô∏è‚É£0Ô∏è‚É£2Ô∏è‚É£4Ô∏è‚É£-1Ô∏è‚É£2Ô∏è‚É£-0Ô∏è‚É£1Ô∏è‚É£ |       ‚úÖ       |   üîó   |
### Comparison with Static Evaluation

We compare the user preference result in interactive evaluation to that of static evaluation by computing the top-k Kendall Tau Distance between rank in static evaluation and that in interactive evaluation:

<KendallTauDistanceChart />

Here are some observations:
1. **None** of the static bench reflects exactly the same rank in interactive eval
2. Ranks on **emotion recognition** and **language detection** benchmarks are most similar to that in interactive eval
3. Ranks on **gender detection** and **nuanced intent (humor, sarcasm) detection** are not so correlated with that in interactive eval
