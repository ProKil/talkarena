import Link from 'next/link'
import Image from 'next/image'
import { Lumiflex } from 'uvcanvas'
import { Button } from "../components/ui/button"
import { MoveRight } from "lucide-react"

import React, { useState } from 'react';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';
import ModelPerformanceChart from '../components/static_performance';
import KendallTauDistanceChart from '../components/interactive_static_correlation';
import ModelComparisonChart from '../components/win_rates';

import { Hero } from "@/components/hero";
import { GradioEmbed } from "@/components/gradio";
import { ResearchTable } from "@/components/tablehero";
import { Callout, Bleed } from 'nextra/components'

# Interactive Evaluation

### User Preference

As an initial effort, we collected a total 5000 votes using Talk Arena for pairwise comparisons among GPT4o,
Gemini-1.5-pro, Typhoon, Qwen2-Audio and DiVA, which are top performing models from the results of static evaluation.
For each of the ten combinations, we collect 500 votes from more than 50 different crowdworkers.

<ModelComparisonChart />

We applied Bradley Terry model to pariwise voting results to get a ranking for the five models tested.
The final result shows a ranking of DiVA, GPT4o, Gemini-1.5-pro, Qwen2-Audio, Typhoon-1.5
(most preferred to less preferred).
<div align="center" style="margin-left: auto; margin-right:auto; width: min(100vw, 600px);">
|     Model     | Bradley Terry Score  | Open-Sourced? | Site  |
|:-------------|:--------------------:|:-------------:|:-----:|
|     ğŸ…DiVA     |         36.0         |       âœ…       |   [ğŸ”—](https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b)   |
|     ğŸ¥ˆGPT4o    |         24.2         |       âŒ       |   [ğŸ”—](https://platform.openai.com/docs/guides/audio)   |
|    ğŸ¥‰Gemini    |         21.7         |       âŒ       |   [ğŸ”—](https://deepmind.google/technologies/gemini/pro/)   |
| 4ï¸âƒ£ Qwen2 Audio |         14.5         |       âœ…       |   [ğŸ”—](https://github.com/QwenLM/Qwen2-Audio)   |
|   5ï¸âƒ£ Typhoon   |          3.6         |       âœ…       |   [ğŸ”—](https://huggingface.co/scb10x/llama-3-typhoon-v1.5-8b-audio-preview)   |
</div>
### Comparison with Static Evaluation

We compare the user preference result in interactive evaluation to that of static evaluation by computing the top-k Kendall Tau Distance between rank in static evaluation and that in interactive evaluation:

<KendallTauDistanceChart />

Here are some observations:
1. **None** of the static bench reflects exactly the same rank in interactive eval
2. Ranks on **emotion recognition** and **language detection** benchmarks are most similar to that in interactive eval
3. Ranks on **gender detection** and **nuanced intent (humor, sarcasm) detection** are not so correlated with that in interactive eval
